MicroExpressionModel(
  (gnn): FacialGNN(
    (gat1): GATConv(2, 8, heads=4)
    (gat2): GATConv(32, 16, heads=4)
    (gat3): GATConv(64, 128, heads=1)
    (norm1): LayerNorm(32, affine=True, mode=graph)
    (norm2): LayerNorm(64, affine=True, mode=graph)
    (norm3): LayerNorm(128, affine=True, mode=graph)
    (dropout): Dropout(p=0.2, inplace=False)
    (proj1): Linear(in_features=32, out_features=256, bias=True)
    (proj2): Linear(in_features=64, out_features=256, bias=True)
    (proj3): Linear(in_features=128, out_features=256, bias=True)
    (alpha_mlp1): Linear(in_features=256, out_features=1, bias=True)
    (alpha_mlp2): Linear(in_features=256, out_features=1, bias=True)
    (alpha_mlp3): Linear(in_features=256, out_features=1, bias=True)
  )
  (transformer): FacialTemporalTransformer(
    (pos_encoding): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): TransformerEncoder(
      (layers): ModuleList(
        (0-3): 4 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (padder): SequencePadder()
  (classifier): Linear(in_features=256, out_features=7, bias=True)
)